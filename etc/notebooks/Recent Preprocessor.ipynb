{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI YouTube Consultant: Preprocessor\n",
    "<br/></br>\n",
    "## Manual\n",
    "* Load and Transform\n",
    "```\n",
    "preprocessor = Preprocessor()\n",
    "data = preprocessor.fit_transform(raw=raw)\n",
    "```\n",
    "* Generate train data\n",
    "```\n",
    "result = preprocessor.get_train_data(data, filter_size, target_size, stride)\n",
    "```                  \n",
    "* TODO: Generate test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:17:33.191705Z",
     "start_time": "2020-11-20T03:17:33.186707Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, date\n",
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/train_raw_LITE.csv'\n",
    "raw = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess finished.\n"
     ]
    }
   ],
   "source": [
    "worker = Preprocessor()\n",
    "worker.fit_transform(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = worker.get_train_data(filter_size=7, target_size=1, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "for i in range(50):\n",
    "    temp = train[0][0]\n",
    "    temp['target'] = train[0][1].values\n",
    "    result = pd.concat([result, temp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('train_LITE.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.IS_TRANSFORM = False\n",
    "        self.logs = dict(added_features=[], scaling_method=[]) # TODO: 트랜스폼 뭐 일어났는지 기록을 담는 곳\n",
    "\n",
    "    def fit_transform(self, raw, return_output=False):\n",
    "        self.raw = raw\n",
    "        self.result = self.raw.copy()\n",
    "        self.object_fts = ['channel', 'title', 'genre','description','date', 'sign_in','is_upload']\n",
    "\n",
    "        #adding more features\n",
    "        self.__n_comment_to_float()\n",
    "        self.__str_to_datetype()\n",
    "        self.__add_is_upload()\n",
    "        self.__add_sub_diff()\n",
    "        self.__add_no_upload_interval()\n",
    "        self.__add_n_hashtag()\n",
    "        \n",
    "        #saving min, max values & scale numerical features\n",
    "        self.numeric_fts = self.result.drop(self.object_fts, axis=1).columns.tolist()\n",
    "        self.__get_min_max_values()\n",
    "        self.__scale()\n",
    "        \n",
    "        #flag to notice that the process has completed and return\n",
    "        self.IS_TRANSFORM = True\n",
    "        \n",
    "        print('Preprocess finished.')\n",
    "        if return_output:\n",
    "            return self.result\n",
    "    \n",
    "    \n",
    "    #ADD OR CONVERTING FEATURES\n",
    "    ##################################\n",
    "    def __n_comment_to_float(self):\n",
    "        '''n_comment 칼럼을 float type으로 변환하고, 댓글사용중지는 0으로 변환'''\n",
    "        self.result.loc[raw['n_comment']=='댓글 사용 중지', 'n_comment'] = 0\n",
    "        self.result['n_comment'] = self.result['n_comment'].astype(float)\n",
    "        \n",
    "        \n",
    "    def __str_to_datetype(self):\n",
    "        '''csv파일 로드시 date 컬럼이 str 타입으로 읽혀진 경우 이를 datetype으로 변환'''\n",
    "        if pd.api.types.is_datetime64_ns_dtype(self.result['date']):\n",
    "            pass\n",
    "        else:\n",
    "            self.result['date'] = pd.to_datetime(self.result['date'])\n",
    "\n",
    "\n",
    "    def __add_is_upload(self):\n",
    "        '''해당 날짜에 영상 업로드가 발생했는지(1) 하지않았는지(0)를 담은 변수 생성'''\n",
    "        self.result = self.result.groupby('channel').apply(lambda x: self._get_is_upload(x)).reset_index(drop=True)\n",
    "        self.logs['added_features'].append('is_upload')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_is_upload(data):\n",
    "        result = data.reset_index(drop=True)\n",
    "        upload_idx = result[result['title'].notnull()].index.tolist()\n",
    "        result['is_upload'] = 0\n",
    "        result.loc[upload_idx, 'is_upload'] = 1\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def __add_sub_diff(self):\n",
    "        '''일간 구독자 변화량 컬럼을 추가하는 함수'''\n",
    "        self.result = self.result.groupby('channel').apply(lambda x: self._whynot(x)).reset_index(drop=True)\n",
    "        self.logs['added_features'].append('sub_diff')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _whynot(data):\n",
    "        result = data.reset_index(drop=True)\n",
    "        result['sub_diff'] = (result['cumul_subs'] - result['cumul_subs'].shift())\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def __add_no_upload_interval(self):\n",
    "        self.result = self.result.groupby('channel').apply(lambda x: self._get_no_upload_interval(x)).reset_index(drop=True)\n",
    "        self.logs['added_features'].append('no_upload_interval')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_no_upload_interval(data):\n",
    "        result = data.reset_index(drop=True)\n",
    "        upload_idx = result[result['is_upload'] == 1].index.tolist()\n",
    "        temp = [0 for i in range(result.shape[0])]\n",
    "        for i in range(len(upload_idx)):\n",
    "            if i == len(upload_idx)-1:\n",
    "                former = upload_idx[i]\n",
    "                temp[former+1:] = [i+1 for i in range(len(temp[former+1:]))]\n",
    "            else:\n",
    "                former, latter = upload_idx[i], upload_idx[i+1]\n",
    "                temp[former+1:latter] = [i+1 for i in range(len(temp[former+1:latter]))]\n",
    "        result['no_upload_interval'] = temp\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def __add_n_hashtag(self):\n",
    "        '''영상별 해시태그 개수를 담은 변수 생성(영상 미업로드시 0)'''\n",
    "        self.result['n_hashtag'] = 0\n",
    "        idx = self.result['description'].notnull()\n",
    "        self.result.loc[idx, 'n_hashtag'] = self.result.loc[idx, 'description'].apply(lambda x: len(x.split('#'))-1)\n",
    "        self.logs['added_features'].append('n_hashtag')\n",
    "    \n",
    "    \n",
    "    def __get_min_max_values(self):\n",
    "        '''Saving min and max values prior to scaling'''\n",
    "        m = (self.result[self.numeric_fts]).min()\n",
    "        M = (self.result[self.numeric_fts]).max()\n",
    "        self.meta = pd.DataFrame([m, M], columns=self.numeric_fts, index=['min','max'])\n",
    "\n",
    "    \n",
    "    def __scale(self):\n",
    "        '''Scaling in between 0 to 1'''\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        temp = scaler.fit_transform(self.result[self.numeric_fts])\n",
    "        self.result.loc[:, self.numeric_fts] = temp\n",
    "        self.logs['scaling_method'] = 'minmax'\n",
    "    \n",
    "    \n",
    "    #GET TRAIN DATA\n",
    "    ##################################\n",
    "    def get_train_data(self, data=None, filter_size=7, target_size=1, stride=1, drop_features=None, label_features=None):\n",
    "        '''모델 학습 환경에 맞는 데이터를 생성하는 함수\n",
    "        Args\n",
    "        ---\n",
    "        data: 데이터, None일 경우 클래스 내 result 인스턴스를 사용\n",
    "        filter_size: 윈도우 사이즈를 7로 설정하여 데이터셋 생성\n",
    "        target_size: 타깃 갯수를 설정\n",
    "        stride: 필터의 stride\n",
    "        drop_features: 제거할 변수, None일 경우 초기 설정된 변수를 제거\n",
    "        label_features: 타깃 정보(인듯?)\n",
    "        \n",
    "        Return\n",
    "        ---\n",
    "        학습 데이터\n",
    "        \n",
    "        '''\n",
    "        #warn in case fit_transform has not yet performed\n",
    "        if self.IS_TRANSFORM==False:\n",
    "            raise NotImplementedError(\"You need to run 'fit_transform' primarily.\")\n",
    "            \n",
    "        # remove channels with few information with respect to filter_size and target_size to extract    \n",
    "        if data is None:\n",
    "            data = self._sift(self.result, filter_size + target_size)\n",
    "        else:\n",
    "            data = self._sift(data, filter_size + target_size)\n",
    "        \n",
    "        #drop_features: features to drop for latter use\n",
    "        #label_features: features to extract as labels\n",
    "        if drop_features is None:\n",
    "            drop_features = ['date', 'genre','title', 'channel', 'description',\t'sign_in', 'current_cumul_view', 'current_n_video', 'current_cumul_subs']\n",
    "        if label_features is None:\n",
    "            label_features = ['sub_diff']\n",
    "        \n",
    "        #return train, label set wrt groups\n",
    "        result = data.groupby('channel').apply(lambda x: self._to_sequential(x, filter_size, target_size, stride, drop_features, label_features)).reset_index(drop=True)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sift(data, filter_size):\n",
    "        '''fillter_size 이상인 채널을 추출하는 함수'''\n",
    "        alive_idx = data['channel'].value_counts() > filter_size # filter_size 이상인애만 \n",
    "        alive_array = alive_idx[alive_idx==True].index #살릴 channel들 array\n",
    "        return data[data['channel'].isin(alive_array)].reset_index(drop=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _to_sequential(data, filter_size, target_size, stride, drop_features, label_features):\n",
    "        data = data.reset_index(drop=True)\n",
    "        idx_list = data.index.tolist()\n",
    "        \n",
    "        train, label = [],[]\n",
    "        for i in range((len(idx_list)-filter_size-target_size)//stride +1):\n",
    "            train_idx = idx_list[i*stride : i*stride + filter_size]\n",
    "            label_idx = idx_list[i*stride + filter_size : i*stride + filter_size + target_size]\n",
    "            train_temp = data.loc[train_idx,:].values.reshape(1,-1)\n",
    "            label_temp = data.loc[label_idx,label_features].values.reshape(1,-1)\n",
    "            \n",
    "            train = train_temp.copy() if i == 0 else np.vstack([train, train_temp])\n",
    "            label = label_temp.copy() if i == 0 else np.vstack([label, label_temp])\n",
    "            \n",
    "        train = pd.DataFrame(train, columns = data.columns.tolist()*filter_size)\n",
    "        label = pd.DataFrame(label, columns = label_features*target_size)\n",
    "        return train.drop(drop_features, axis=1), label\n",
    "    \n",
    "    \n",
    "    #INVERSE SCALE\n",
    "    ##################################\n",
    "    def split_days(self, pred):\n",
    "        idx_list = pred.columns.tolist()\n",
    "        days = pred.shape[1] // len(idx_list)\n",
    "        for i in range(days):\n",
    "            pred[idx_list].iloc[:, i*L : (i+1) * L].apply(lambda x: _inverse_scale(x))\n",
    "            \n",
    "    def _inverse_scale(self, pred):\n",
    "        idx_list = pred.columns.tolist()\n",
    "        Min = self.meta[idx_list].loc['min']\n",
    "        Max = self.meta[idx_list].loc['max']\n",
    "        return (Max - Min)*pred[idx_list] + Min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "lite_path = '/home/mskang/CapstoneUOS/raw/train_raw_LITE.csv'\n",
    "full_path = '/home/mskang/CapstoneUOS/raw/meta_trend_data(201130).csv'\n",
    "lite = pd.read_csv(lite_path)\n",
    "full = pd.read_csv(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:39:35.722245Z",
     "start_time": "2020-11-20T03:39:35.718246Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "lite_data = preprocessor.fit_transform(lite)\n",
    "#full_data = preprocessor.fit_transform(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = preprocessor.get_train_data(lite_data, 30, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pending\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:39:32.423098Z",
     "start_time": "2020-11-20T03:39:32.386099Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    def merge_in_samedate(self): \n",
    "        \"같은 날에 올린 영상 여러개 하루치로 만들기\"\n",
    "        \n",
    "        # 1. feature 추가 - 하루에 올린 영상 개수 (안 올린날은 0)\n",
    "        df_video_num=self.add_video_num_in_same_days() #  하루 올린 영상 개수에 대한 series\n",
    "        \n",
    "       # 2. 모든 feature에 대해 하루로 합치기  -->  하루에 올린 3개의 영상을 1개의 영상으로 합친다 (평균이용, 분산feature 추가)\n",
    "        col_str=['sign_in','title','description','genre'] # 문자열 feature들\n",
    "        col_std_mean=['duration','video_n_view', 'n_comment', 'n_like', 'n_dislike'] # 평균 + 표준편차까지 추가 할 feature\n",
    "        col_residue=[x for x in list(self.result.columns) if x not in col_str+col_std_mean+['channel','date']] #그 외 ex-누적 조회수, 일일조회수, 업로드 간격 등\n",
    "        \n",
    "        df_str=self.result.groupby(['channel','date'])[col_str].first() #문자열 feature들은 첫번째 동영상을 따른다 (수정 필요할 수도)\n",
    "        df_std=self.result.groupby(['channel','date'])[col_std_mean].std() # 표준편차에 대한 featrue 생성\n",
    "        df_mean=self.result.groupby(['channel','date'])[col_std_mean].mean() # 평균으로 feature 생성\n",
    "        df_residue=self.result.groupby(['channel','date'])[col_residue].mean() # 그 외 feature ex)누적조회수는 같은 날짜 내에서 모두 동일\n",
    "        \n",
    "        df_std.columns=[x+'_std' for x in list(df_std.columns)] # feature 이름 수정('~~_std')\n",
    "        \n",
    "        col_array=col_str+[df_video_num.name]+[*sum(zip(list(df_mean.columns),list(df_std.columns)),())]+col_residue # feature 순서 섞기 + 조정\n",
    "        self.result=pd.concat([df_str,df_video_num,df_mean,df_std,df_residue],axis=1)\n",
    "        self.result=self.result[col_array] # 순서 정렬\n",
    "        self.result=self.result.reset_index() # index 돌려놓기.\n",
    "    \n",
    "        \n",
    "    def add_video_num_in_same_days(self):\n",
    "        \"하루에 올린 영상 개수\"\n",
    "        grouped=self.result.groupby(['channel','date'])\n",
    "        video_num= grouped.size() # 하루에 올린 영상 개수에 대한 Series, \n",
    "        video_num[grouped['title'].apply(lambda x : x.values[0]).isnull().values]=0 #  동영상 없는 채널은 0으로. \n",
    "        video_num.name='video_num_per_day'\n",
    "        return video_num # 하루에 올린 영상 개수에 대한 Series를 return값으로. \n",
    "\n",
    "    def add_title_length(self):\n",
    "        \"제목 길이\"\n",
    "        self.result['title_length']=self.result['title'].apply(lambda x : len(x) if x is not np.nan else (np.nan))\n",
    "\n",
    "    def add_like_per_view(self):\n",
    "        \" 영상 호감도 - 조회수 대비 좋아요 수  - 단순히 좋아하는 사람의 비율 : 구독자 상승은 좋아하는 사람만 많으면 됨\"\n",
    "        self.result['like_per_view']=self.result['n_like']/self.result['video_n_view']\n",
    "        \n",
    "    def add_dislike_per_view(self):\n",
    "        \"영상 비호감도 - 조회수 대비 싫어요 수 - 단순히 싫어하는 사람의 비율\"\n",
    "        self.result['dislike_per_view']=self.result['n_dislike']/self.result['video_n_view']\n",
    "    \n",
    "    def add_interest_per_view(self):\n",
    "        \" 영상 참여도 - 조회수 대비 (좋아요+싫어요+댓글) 수 - 영상에 관심이 있는 정도\"\n",
    "        self.result['interest_per_view']=(self.result['n_like']+self.result['n_dislike']+self.result['n_comment'])/self.result['video_n_view']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTER\n",
    "### Inverse Scaler\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = test.fit_transform(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = result[0][0].iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label.index.unique().tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
