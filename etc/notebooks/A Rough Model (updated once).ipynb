{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Namin/Documents/CAP/CapstoneUOS/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Read the files.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(past, future):\n",
    "    \"\"\"\n",
    "    result[:][0], result[:][1] 로 전처리된 파일 읽는 함수\n",
    "    \"\"\"\n",
    "    df_past = pd.read_excel(past, header=0, index_col=0)\n",
    "    df_future = pd.read_excel(future, header=0, index_col=0)\n",
    "    \n",
    "    return df_past, df_future\n",
    "\n",
    "n_past = 180\n",
    "n_future = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Generate training, validation and test datasets.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tvt(df_past, df_future, predict_features_list, BATCH_SIZE=64):\n",
    "    \"\"\"\n",
    "    train, val, test 데이터 생성하는 함수\n",
    "    \"\"\"\n",
    "    predict_features = predict_features_list # 예측하고 싶은 변수들만\n",
    "    df_future = df_future[predict_features]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_past, df_future, test_size=0.3, random_state=7)\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=7)\n",
    "\n",
    "    x_train_copy = x_train.values.reshape(x_train.shape[0], X_train.shape[1], 1)\n",
    "    y_train_copy = y_train.values\n",
    "    x_val_copy = x_val.values.reshape(x_val.shape[0], x_val.shape[1], 1)\n",
    "    y_val_copy = y_val.values\n",
    "    x_test_copy = x_test.values.reshape(y_test.shape[0], y_test.shape[1], 1)\n",
    "    y_test_copy = y_test.values\n",
    "\n",
    "    # 주석 처리\n",
    "    print(\"x_train's shape: \", x_train_copy.shape) \n",
    "    print(\"y_train's shape: \", y_train_copy.shape) \n",
    "    print(\"x_val's shape: \", x_val_copy.shape)\n",
    "    print(\"y_val's shape: \", y_val_copy.shape)\n",
    "    print(\"x_test's shape: \", x_test_copy.shape)\n",
    "    print(\"y_test's shape: \", y_test_copy.shape)\n",
    "    \n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train_copy, y_train_copy))\n",
    "    train_data = train_data.cache().batch(BATCH_SIZE).repeat()\n",
    "    val_data = tf.data.Dataset.from_tensor_slices((x_val_copy, y_val_copy))\n",
    "    val_data = val_data.cache().batch(BATCH_SIZE).repeat()\n",
    "    \n",
    "    return x_train, y_train, x_test_copy, y_test_copy, train_data, val_data, BATCH_SIZE, df_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Plot some graphs for training samples.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_graphs(df_past, m, x_train, y_train):\n",
    "    \"\"\"\n",
    "    어떤 변수들로 어떤 정보를 예측하고 싶은지 그래프로 나타내주는 함수\n",
    "    \"\"\"\n",
    "    past_features = df_past.columns.unique()\n",
    "    \n",
    "    for i in range(m):\n",
    "        fig = plt.figure(figsize=(5*len(past_features), 5)) \n",
    "    \n",
    "        for j in range(len(past_features)):\n",
    "            plt.subplot(m, len(past_features), j+1)\n",
    "            plt.plot(x_train.iloc[i][n_past*j:n_past*(j+1)], 'g', label=\"past\")\n",
    "            \n",
    "            for k in range(len(predict_features)):\n",
    "                if predict_features[k] == past_features[j]:\n",
    "                    plt.plot(y_train.iloc[i][n_past*j:n_past*(j+1)], 'r', label=\"future (GOAL)\") # 예측해야 할 추이\n",
    "            \n",
    "            plt.title(past_features[j], fontdict={'size': 20}) # 'color': white\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Make a (rough) model.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rnn(n_future):\n",
    "    \"\"\"\n",
    "    RNN 모델과 callback_list을 리턴하는 함수\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(units=16,\n",
    "                                   return_sequences=True,\n",
    "                                   input_shape=(x_train_copy.shape[-2:])))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1000))\n",
    "    model.add(tf.keras.layers.Dense(n_future))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['loss'])\n",
    "    \n",
    "    # 주석 처리\n",
    "    model.summary()\n",
    "    \n",
    "    callback_list = [ModelCheckpoint(filepath='rough_rnn_checkpoint.h5',\n",
    "                                     monitor='val_loss',\n",
    "                                     save_best_only=True),\n",
    "                     TensorBoard(log_dir='rough_rnn_logs/{}'.format(time.asctime()))]\n",
    "    \n",
    "    return model, callback_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6281440796c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m history = model.fit(train_data,\n\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-6b5cfc0e75a2>\u001b[0m in \u001b[0;36mrnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     model.add(tf.keras.layers.LSTM(units=16,\n\u001b[1;32m      4\u001b[0m                                    \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                    input_shape=(x_train_copy.shape[-2:])))\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train_copy' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "model = rnn()\n",
    "\n",
    "history = model.fit(train_data,\n",
    "                    epochs=EPOCHS,\n",
    "                    steps_per_epoch=x_train_copy.shape[0] // BATCH_SIZE, # training samples / batch size\n",
    "                    validation_data=val_data,\n",
    "                    validation_steps=x_test_copy.shape[0] // BATCH_SIZE,\n",
    "                    callbacks=callback_list) # validation samples / batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Show the results.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(EPOCHS, history):\n",
    "    \"\"\"\n",
    "    Train loss와 val loss를 그래프로 나타내주는 함수\n",
    "    \"\"\"\n",
    "    epochs = np.arange(1, EPOCHS+1)\n",
    "    fig, axes = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    plt.plot(epochs, history.history['loss'], label='train_loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('epochs', fontdict={'size': 10})\n",
    "    plt.ylabel('loss', fontdict={'size': 10})\n",
    "    axes.tick_params(axis='x')\n",
    "    axes.tick_params(axis='y')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Use the model to predict any test datas.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_future(x_test_copy):\n",
    "    \"\"\"\n",
    "    Self-explanatory\n",
    "    \"\"\"\n",
    "    test_predict = model.predict(x_test_copy)\n",
    "    \n",
    "    return test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_future(df_future, n_past, n_future, x_test_copy, y_test_copy, test_predict):\n",
    "    \"\"\"\n",
    "    잘 예측이 되었는지 그래프로 확인하는 함수\n",
    "    \"\"\"\n",
    "    future_features = df_future.columns.unique()\n",
    "    \n",
    "    for k in range(len(x_test_copy.shape[0])):\n",
    "        dates = [i for i in range(n_past+n_future)]\n",
    "        fig, axes = plt.subplots(figsize=(5*len(future_features), 5))\n",
    "        \n",
    "        for j in range(len(future_features)):\n",
    "            plt.subplot(k, len(past_features), j+1)\n",
    "        \n",
    "            plt.plot(dates[:n_past], x_test_copy[k][j*n_past:(j+1)*n_past], 'g', label='past')\n",
    "            plt.plot(dates[n_past:], y_test_copy[k][j*n_past:(j+1)*n_past], 'r', label='actual future')\n",
    "            plt.plot(dates[n_past:], test_predict[k][j*n_past:(j+1)*n_past], 'b', label='predicted future')\n",
    "            \n",
    "            plt.title(future_features[j], fontdict={'size': 20}) # , 'color': 'white'\n",
    "            plt.xlabel('Time', fontdict={'size': 10})\n",
    "            plt.ylabel('Subscribers', fontdict={'size': 10})\n",
    "            axes.tick_params(axis='x') # colors='white'\n",
    "            axes.tick_params(axis='y')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(y_test_copy, test_predict):\n",
    "    \"\"\"\n",
    "    Test dataset에서의 손실함수(MSE, MAE)\n",
    "    \"\"\"\n",
    "    diff = np.subtract(y_test_copy, test_predict)\n",
    "    sq = np.square(diff)\n",
    "    ab = np.abs(diff)\n",
    "    mse = np.mean(sq)\n",
    "    mae = np.mean(ab)\n",
    "    \n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
