{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Channel Trend Crawling\n",
    "* Author: 고지형, [iloveslowfood](https://github.com/iloveslowfood)\n",
    "* 채널의 구독자 추이와 조회수 추이를 수집한다.\n",
    "* 최근 360일까지 수집이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_list = pd.read_csv('../raw/channel_list_지형.csv')['channel'].tolist()\n",
    "save_path = '../raw/#2. subscribe trend data' # 파일을 저장할 폴더\n",
    "driver_path = './drivers/chromedriver.exe' # 크롬드라이버 저장 경로\n",
    "\n",
    "batch_size = len(channel_list) // 20\n",
    "subsample = [channel_list[batch_size*i:batch_size*(i+1)] for i in range(20)] + [channel_list[batch_size*20:]]\n",
    "\n",
    "crawler = TrendCrawler(driver_path, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crawler.work(subsample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrendCrawler:\n",
    "    URL = 'https://kr.noxinfluencer.com/'\n",
    "    def __init__(self, driver_path, save_path):\n",
    "        self.driver_path = driver_path\n",
    "        self.save_path = save_path\n",
    "    \n",
    "    def work(self, channel_list):\n",
    "        '''일해라 로봇'''\n",
    "        try:\n",
    "            os.mkdir(os.path.join(self.save_path))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.driver = webdriver.Chrome(self.driver_path)\n",
    "        self.driver.get(self.URL)\n",
    "        for channel_name in channel_list:\n",
    "            try:\n",
    "                self.into_channel(channel_name)\n",
    "                sub_trend = self.get_trend(channel_name, trend_type='sub_trend')\n",
    "                view_trend = self.get_trend(channel_name, trend_type='view_trend')\n",
    "                result = self.wrap(sub_trend, view_trend)\n",
    "\n",
    "                file_name = f'ChannelTrend_{self.correct_file_name(channel_name)}.csv'\n",
    "                result.to_csv(os.path.join(self.save_path, file_name), index=False)\n",
    "                print(f'{file_name} saved.\\n')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    def get_trend(self, channel_name, trend_type):\n",
    "        '''추이를 크롤링하는 함수'''\n",
    "        graph_elements = self.grope(trend_type)\n",
    "\n",
    "        if trend_type == 'sub_trend':\n",
    "            date_list = []\n",
    "            n_sub_list = []\n",
    "            print(f'Getting subscribe trend from {channel_name}...', end='\\t')\n",
    "            for n in range(1, graph_elements['date_interval']):\n",
    "                self.move_cursor(n, graph_elements['start_point'], graph_elements['element'], graph_elements['pix'])\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                info = soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()\n",
    "                if '획기적' in info:\n",
    "                    try:\n",
    "                        try:\n",
    "                            date = (pd.to_datetime(date) + timedelta(1)).strftime('%Y-%m-%d')\n",
    "                        except:\n",
    "                            date = np.nan\n",
    "                    except:\n",
    "                        date = pd.to_datetime('1900-01-01')\n",
    "                    n_sub = self.calc_n_str(info.split(':')[-1].split('구독자 ')[-1])\n",
    "                else:\n",
    "                    date = info[:10]\n",
    "                    n_sub = self.calc_n_str(info[10:])\n",
    "                date_list.append(date)\n",
    "                n_sub_list.append(n_sub)\n",
    "\n",
    "            sub_trend = pd.DataFrame(dict(date=date_list, subscriber=n_sub_list))\n",
    "            sub_trend['date'] = pd.to_datetime(sub_trend['date'])\n",
    "            sub_trend = self.correct_timeline(sub_trend)\n",
    "            sub_trend = sub_trend.drop_duplicates(ignore_index=True)\n",
    "\n",
    "            return sub_trend\n",
    "        else:\n",
    "            date_list = []\n",
    "            n_view_list = []\n",
    "            print(f'Getting view trend from {channel_name}...')\n",
    "            for n in range(1, graph_elements['date_interval']):\n",
    "                self.move_cursor(n, graph_elements['start_point'], graph_elements['element'], graph_elements['pix'])\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                info = soup.find('div', id=\"channel-history-view-chart\").find_all('div')[-1].get_text()\n",
    "                date = info[:10]\n",
    "                if '조회수' in info[10:]:\n",
    "                    n_view = self.calc_n_str(info[10:].strip().split('조회수')[0])\n",
    "                else:\n",
    "                    n_view = self.calc_n_str(info[10:])\n",
    "\n",
    "                date_list.append(date)\n",
    "                n_view_list.append(n_view)\n",
    "\n",
    "            view_trend = pd.DataFrame(dict(date=date_list, view=n_view_list))\n",
    "            view_trend['date'] = pd.to_datetime(view_trend['date'])\n",
    "            view_trend = self.correct_timeline(view_trend)\n",
    "            view_trend = view_trend.drop_duplicates(ignore_index=True)\n",
    "            \n",
    "            return view_trend\n",
    "        \n",
    "    def grope(self, trend_type):\n",
    "        '''추출할 트렌드의 날짜 범위, 그래프 내 좌표 범위를 추출하는 함수'''\n",
    "        if trend_type == 'sub_trend':\n",
    "            wait = WebDriverWait(self.driver, 10)\n",
    "            element = wait.until(lambda x: x.find_element_by_xpath('//*[@id=\"channel-history-sub-chart\"]/div[1]/canvas'))\n",
    "        elif trend_type == 'view_trend':\n",
    "            wait = WebDriverWait(self.driver, 10)\n",
    "            element = wait.until(lambda x: x.find_element_by_xpath('//*[@id=\"channel-history-view-chart\"]/div[1]/canvas'))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        # 그래프 위치 찾기\n",
    "        loc = element.location\n",
    "        size = element.size\n",
    "        origin = element.size['width'] // 2\n",
    "        self.move_cursor(0, origin, element)\n",
    "\n",
    "        start_origin = -origin + 60\n",
    "        end_origin = origin - 20\n",
    "        \n",
    "        start_point, start_date = self.find_edges(element, start_origin, trend_type, 'start')\n",
    "        end_point, end_date = self.find_edges(element, end_origin, trend_type, 'end')\n",
    "        pix_interval = end_point - start_point\n",
    "        date_interval = (end_date - start_date).days\n",
    "        pix = pix_interval / date_interval\n",
    "        \n",
    "        return dict(date_interval=date_interval, start_point=start_point, element=element, pix=pix)\n",
    "    \n",
    "    def find_edges(self, element, origin, trend_type, option, margin=-2) -> ('point', 'date'):\n",
    "        '''그래프의 끝과 끝 위치값을 탐색하는 함수'''\n",
    "        if trend_type == 'sub_trend':\n",
    "            self.move_cursor(0, 0, element)\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            standard = soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()[:10]\n",
    "            if option=='start':\n",
    "                compare = None\n",
    "                while True:\n",
    "                    self.move_cursor(n_offset=0, origin=origin, element=element)\n",
    "                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                    temp = soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()\n",
    "                    compare = temp[:10]\n",
    "\n",
    "                    if compare != standard:\n",
    "                        if '획기적' not in compare: # '획기적 사건'이 아닌 일반적인 날짜\n",
    "                            origin += margin\n",
    "                            start_date = pd.to_datetime(soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()[:10])\n",
    "                            start_point = origin\n",
    "                            continue\n",
    "\n",
    "                        else: # '획기적 사건'이 나올 경우\n",
    "                            while True:\n",
    "                                self.move_cursor(n_offset=0, origin=origin, element=element)\n",
    "                                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                                pseudo_start = soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()[:10]\n",
    "                                start_point = origin\n",
    "                                if '획기적' not in pseudo_start:\n",
    "                                    start_date = pd.to_datetime(pseudo_start[:10]) - timedelta(1)\n",
    "                                    break\n",
    "                                else:\n",
    "                                    origin -= margin\n",
    "                                    continue\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                return start_point, start_date\n",
    "\n",
    "            else:\n",
    "                compare = None\n",
    "                while True:\n",
    "                    self.move_cursor(n_offset=0, origin=origin, element=element)\n",
    "                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                    temp = soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()\n",
    "                    compare = temp[:10]\n",
    "\n",
    "                    if compare != standard:\n",
    "                        if '획기적' not in compare: # '획기적 사건'이 아닌 일반적인 날짜\n",
    "                            origin -= margin\n",
    "                            end_date = pd.to_datetime(soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()[:10])\n",
    "                            end_point = origin\n",
    "                            continue\n",
    "\n",
    "                        else: # '획기적 사건'이 나올 경우\n",
    "                            while True:\n",
    "                                self.move_cursor(0, origin, element)\n",
    "                                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                                pseudo_end = soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()[:10]\n",
    "                                end_point = origin\n",
    "                                if '획기적' not in pseudo_end:\n",
    "                                    end_date = pd.to_datetime(pseudo_end[:10]) - timedelta(1)\n",
    "                                    break\n",
    "                                else:\n",
    "                                    origin += margin\n",
    "                                    continue\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                return end_point, end_date\n",
    "\n",
    "        elif trend_type == 'view_trend':\n",
    "            self.move_cursor(0, 0, element)\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            standard = soup.find('div', id=\"channel-history-view-chart\").find_all('div')[-1].get_text()[:10]\n",
    "            if option=='start':\n",
    "                compare = None\n",
    "                while True:\n",
    "                    self.move_cursor(n_offset=0, origin=origin, element=element)\n",
    "                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                    temp = soup.find('div', id=\"channel-history-view-chart\").find_all('div')[-1].get_text()\n",
    "                    compare = temp[:10]\n",
    "\n",
    "                    if compare != standard:\n",
    "                        origin += margin\n",
    "                        start_date = pd.to_datetime(compare)\n",
    "                        start_point = origin\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                return start_point, start_date\n",
    "\n",
    "            else:\n",
    "                compare = None\n",
    "                while True:\n",
    "                    self.move_cursor(n_offset=0, origin=origin, element=element)\n",
    "                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                    temp = soup.find('div', id=\"channel-history-view-chart\").find_all('div')[-1].get_text()\n",
    "                    compare = temp[:10]\n",
    "\n",
    "                    if compare != standard:\n",
    "                        origin -= margin\n",
    "                        end_date = pd.to_datetime(compare)\n",
    "                        end_point = origin\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                return end_point, end_date\n",
    "            \n",
    "    def move_cursor(self, n_offset, origin, element, pix=2):\n",
    "        action = webdriver.common.action_chains.ActionChains(self.driver)\n",
    "        action.move_to_element(element)\n",
    "        action.move_by_offset(origin + pix*n_offset, 0)\n",
    "        action.perform()\n",
    "        \n",
    "    def into_channel(self, channel_name):\n",
    "        CHANNEL = self.driver.find_element_by_xpath('//*[@id=\"header-search-input\"]')\n",
    "        CHANNEL.clear()\n",
    "        CHANNEL.send_keys(channel_name)\n",
    "        time.sleep(0.5)\n",
    "        CHANNEL.send_keys(Keys.ENTER)\n",
    "        \n",
    "        patience = 0\n",
    "        while True:\n",
    "            try:\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                link = self.URL[:-1] + soup.find('div', class_='result').find('a', class_=\"channel-name ellipsis\", href=True)['href']\n",
    "                break\n",
    "            except: \n",
    "                if patience == 10:\n",
    "                    raise NotImplementedError()\n",
    "                time.sleep(0.5)\n",
    "                patience += 1\n",
    "        \n",
    "        compare = soup.find('a', class_='channel-name ellipsis').get_text().strip()\n",
    "        \n",
    "        if compare != channel_name.strip():\n",
    "            print(f\"Could not found channel '{channel_name.strip()}'\")\n",
    "            raise NameError()\n",
    "            \n",
    "        self.driver.get(link)\n",
    "        \n",
    "    def wrap(self, sub_trend, view_trend):\n",
    "        '''구독자 추이, 조회수 추이를 병합하는 함수'''\n",
    "        result = sub_trend.merge(view_trend, how='outer', on='date').sort_values(by='date', ignore_index=True)\n",
    "        result = self.imputate(result)\n",
    "        result['subscriber'] = result['subscriber'].astype(int)\n",
    "        result['view'] = result['view'].astype(int)\n",
    "        return result    \n",
    "        \n",
    "    @staticmethod\n",
    "    def calc_n_str(str_n):\n",
    "        if '만' in str_n:\n",
    "            num = str_n.split('만')[0]\n",
    "            return int(float(num) * 10000)\n",
    "        elif '천' in str_n:\n",
    "            num = str_n.split('천')[0]\n",
    "            return int(float(num) * 1000)\n",
    "        else:\n",
    "            return int(float(str_n))\n",
    "        \n",
    "    @staticmethod\n",
    "    def correct_file_name(title):\n",
    "        invalid_file_name_list = ['\\\\', '/', ':', '*', '?', '\"', '<', '>', '|']\n",
    "        for inv in invalid_file_name_list:\n",
    "            if inv in title:\n",
    "                title = title.replace(inv, '')\n",
    "        return title\n",
    "\n",
    "    @staticmethod\n",
    "    def correct_timeline(data):\n",
    "        idx_to_correct = data[data['date'] == pd.to_datetime('1900-01-01')].index.tolist()\n",
    "        if idx_to_correct:\n",
    "            idx_shift = list(np.array(idx_to_correct) + 1)\n",
    "            data.loc[idx_to_correct, 'date'] = (data.loc[idx_shift, 'date'] - timedelta(1)).values\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def imputate(df):\n",
    "        detect = df.isnull().sum()\n",
    "        missing_cols = detect[detect != 0].index.tolist()\n",
    "        for col in missing_cols:\n",
    "            missing_idx = df[df[col].isnull()].index.tolist()\n",
    "            for m in missing_idx:\n",
    "                if m == 0:\n",
    "                    upper_fill = df.loc[m+1:, col]\n",
    "                    upper_bound = upper_fill[upper_fill.notnull()].tolist()[0]\n",
    "                    fill_value = int(upper_bound)\n",
    "                    pass\n",
    "                elif m == df.shape[0]-1:\n",
    "                    lower_fill = df.loc[:m-1, col]\n",
    "                    lower_bound = lower_fill[lower_fill.notnull()].tolist()[-1]\n",
    "                    fill_value = int(lower_bound)\n",
    "                else:\n",
    "                    lower_fill = df.loc[:m-1, col]\n",
    "                    upper_fill = df.loc[m+1:, col]\n",
    "                    lower_bound = lower_fill[lower_fill.notnull()].tolist()[-1]\n",
    "                    upper_bound = upper_fill[upper_fill.notnull()].tolist()[0]\n",
    "                    fill_value = int(np.mean([lower_bound, upper_bound]))\n",
    "                df.loc[m, col] = fill_value\n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
