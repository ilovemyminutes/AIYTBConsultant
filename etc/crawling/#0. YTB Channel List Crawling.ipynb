{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Channel List Crawling\n",
    "* Author: 고지형, iloveslowfood\n",
    "* [The Youtube Channel Crawler](https://www.channelcrawler.com/)로부터 채널명 및 url을 크롤링한다.\n",
    "* Creation Date(채널 가입일)을 조정하여 크롤링할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 300)\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual\n",
    "  \n",
    "I. 크롤러 객체 생성\n",
    "```python\n",
    "crawler = Crawler(\n",
    "    driver_path=driver_path, # 크롬드라이버 경로\n",
    "    url=url, # 채널크롤러 웹사이트 url\n",
    "    save_path=save_path # 결과 파일을 저장할 경로\n",
    ")\n",
    "```\n",
    "II. 크롤링 시작\n",
    "```python\n",
    "crawler.work(\n",
    "    date_interval=date_interval, # Creation Date\n",
    "    filter_size=filter_size # 한번에 훑을 날짜 간격(2 또는 3 권장, 기본값은 2)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.channelcrawler.com/' # YTCC url\n",
    "driver_path = 'drivers/chromedriver.exe' # 드라이버 경로\n",
    "save_path = '../raw/#3. channel list'\n",
    "date_interval = (565, 655) # 최소/최대 Creation Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = YTCCCrawler(driver_path, url, save_path)\n",
    "crawler.work(date_interval, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YTCCCrawler:\n",
    "    def __init__(self, driver_path, url, save_path='../raw/#3. channel list'):\n",
    "        self.driver_path = driver_path\n",
    "        self.url = url\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def work(self, date_interval, filter_size=2):\n",
    "        self.driver = webdriver.Chrome(self.driver_path)\n",
    "        stride = filter_size\n",
    "        print(f'<Creation Date: {date_interval[0]}~{date_interval[-1]}>')\n",
    "        dates = [i for i in range(date_interval[0], date_interval[-1]+1)]\n",
    "        for edge_idx in range(0, len(dates), stride):\n",
    "            min_date = min(dates[edge_idx : edge_idx+filter_size])\n",
    "            max_date = max(dates[edge_idx : edge_idx+filter_size])\n",
    "            if max_date - min_date > 0:\n",
    "                print(f'\\tOngoing: {min_date}~{max_date}', end='\\t')\n",
    "                self.initialize(min_date, max_date) # 구간 초기화\n",
    "                self.get_info(min_date, max_date) # 구간 내 정보 수집\n",
    "            else:\n",
    "                pass\n",
    "        self.driver.close()\n",
    "            \n",
    "    def initialize(self, min_date, max_date):\n",
    "        self.driver.get(self.url)\n",
    "        self.click(option='clear')\n",
    "        self.set_condition(min_date, max_date)\n",
    "        self.click(option='search')\n",
    "        \n",
    "    def get_info(self, min_date, max_date):\n",
    "        channel_url_list = []\n",
    "        channel_title_list = []\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        \n",
    "        page_num = 1\n",
    "        end_num = 999\n",
    "        need_check = True\n",
    "        page_id = self.driver.current_url + '/page:'\n",
    "        try:\n",
    "            pseudo_end_num = int(soup.find('div', class_=\"col-xs-12 text-center\").find_all('a', href=True)[-2]['href'].split(':')[-1])\n",
    "            next_page = int(soup.find('div', class_=\"col-xs-12 text-center\").find_all('a', href=True)[-1]['href'].split(':')[-1])\n",
    "        except:\n",
    "            pseudo_end_num = 5\n",
    "            next_page = 10\n",
    "        \n",
    "        while True:\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            channel_soups = soup.find_all('div', class_=\"channel col-xs-12 col-sm-4 col-lg-3\")\n",
    "            for channel in channel_soups:\n",
    "                channel_url_list.append(channel.find('a', href=True)['href'])\n",
    "                channel_title_list.append(channel.find('a', href=True)['title'])\n",
    "                \n",
    "            # 다음 페이지로\n",
    "            if page_num < end_num:\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                try:\n",
    "                    pseudo_end_num = int(soup.find('div', class_=\"col-xs-12 text-center\").find_all('a', href=True)[-2]['href'].split(':')[-1])\n",
    "                    next_num = int(soup.find('div', class_=\"col-xs-12 text-center\").find_all('a', href=True)[-1]['href'].split(':')[-1])\n",
    "                    if need_check:\n",
    "                        if pseudo_end_num == next_num:\n",
    "                            end_num = pseudo_end_num\n",
    "                            need_check = False\n",
    "                        else:\n",
    "                            end_num = 15\n",
    "                    page_num += 1\n",
    "                    self.driver.get(page_id + str(page_num))\n",
    "                except:\n",
    "                    page_num += 1\n",
    "                    self.driver.get(page_id + str(page_num))\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        result = pd.DataFrame(dict(channel=channel_title_list, url=channel_url_list))\n",
    "        result.drop_duplicates(ignore_index=True, inplace=True)\n",
    "        \n",
    "        # 파일 저장\n",
    "        today = date.today().strftime('%y%m%d')\n",
    "        file_name = f'YTCC_{today}_{min_date, max_date}.csv'\n",
    "        path = os.path.join(self.save_path, file_name)\n",
    "        result.to_csv(path, index=False)\n",
    "        print(f\"'{file_name}' saved.\")\n",
    "        \n",
    "    def set_condition(self, min_date, max_date):\n",
    "        placeholder_min_date = self.driver.find_element_by_xpath('//*[@id=\"queryMinPublishedOn\"]')\n",
    "        placeholder_max_date = self.driver.find_element_by_xpath('//*[@id=\"queryMaxPublishedOn\"]')\n",
    "        placeholder_min_sub = self.driver.find_element_by_xpath('//*[@id=\"queryMinSubs\"]')\n",
    "\n",
    "        placeholder_min_date.clear()\n",
    "        placeholder_max_date.clear()\n",
    "        placeholder_min_sub.clear()\n",
    "\n",
    "        placeholder_min_date.send_keys(min_date)\n",
    "        placeholder_max_date.send_keys(max_date)\n",
    "        placeholder_min_sub.send_keys(100)\n",
    "\n",
    "    def click(self, option):\n",
    "        if option == 'clear':\n",
    "            clear = self.driver.find_element_by_xpath('//*[@id=\"queryIndexForm\"]/div[2]/div[1]/div[4]/div/div/div[1]/div/span[1]/i')\n",
    "            clear.click()\n",
    "        elif option == 'search':\n",
    "            search = self.driver.find_element_by_xpath('/html/body/div/div[3]/div/div[1]/div[2]/div/button')\n",
    "            search.click()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
