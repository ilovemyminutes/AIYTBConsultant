{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Smart Crawling\n",
    "* Author: 고지형, [iloveslowfood](https://github.com/iloveslowfood)\n",
    "* 유튜브 채널의 기본 정보와 구독자 추이, 조회수 추이를 수집한다.\n",
    "* 기본 정보: 영상 조회수, 영상 길이 등\n",
    "* 구독자 추이, 조회수 추이는 최근 360일까지 수집된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual\n",
    "* INPUT: YTCCCrawler로 수집된 채널 URL 데이터\n",
    "* 크롤러 객체 생성\n",
    "```python\n",
    "crawler = SmartCrawler(\n",
    "    driver_path, # 크롬드라이버 경로\n",
    "    save_path # 수집한 데이터의 저장 경로\n",
    ")\n",
    "```\n",
    "* 크롤러 실행\n",
    "```python\n",
    "crawler.work(channel_list) # channel_list: YTCCCrawler에 의해 수집된 URL 데이터\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_list = pd.read_csv('../raw/channel_list_지형.csv')\n",
    "save_path = '../raw/test' # 파일을 저장할 폴더\n",
    "driver_path = './drivers/chromedriver.exe' # 크롬드라이버 저장 경로\n",
    "\n",
    "# generate Crawler object\n",
    "crawler = SmartCrawler(driver_path, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.work(channel_list.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartCrawler:\n",
    "    def __init__(self, driver_path, save_path):\n",
    "        self.driver_path = driver_path\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def work(self, channel_list: '채널명, 채널 url 컬럼을 지닌 데이터프레임'):\n",
    "        for _, channel in channel_list.iterrows():\n",
    "            name, url = channel['channel'], channel['url']\n",
    "            channel_save_path = os.path.join(self.save_path, self.correct_file_name(name))\n",
    "            try:\n",
    "                os.mkdir(channel_save_path)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            trend_crawler = TrendCrawler(self.driver_path, channel_save_path)\n",
    "            meta_crawler = MetaCrawler(self.driver_path, channel_save_path)\n",
    "\n",
    "            try:\n",
    "                trend_crawler.work(name=name)\n",
    "                meta_crawler.work(url=url)\n",
    "            except:\n",
    "                continue    \n",
    "            \n",
    "            \n",
    "    @staticmethod\n",
    "    def correct_file_name(title):\n",
    "        invalid_file_name_list = ['\\\\', '/', ':', '*', '?', '\"', '<', '>', '|']\n",
    "        for inv in invalid_file_name_list:\n",
    "            if inv in title:\n",
    "                title = title.replace(inv, '')\n",
    "        return title\n",
    "\n",
    "\n",
    "class TrendCrawler:\n",
    "    URL = 'https://kr.noxinfluencer.com/'\n",
    "    def __init__(self, driver_path, save_path):\n",
    "        self.driver_path = driver_path\n",
    "        self.save_path = save_path\n",
    "    \n",
    "    def work(self, name: '수집할 채널의 이름'):\n",
    "        '''일해라 로봇'''\n",
    "        \n",
    "        self.driver = webdriver.Chrome(self.driver_path)\n",
    "        self.driver.get(self.URL)\n",
    "        self.into_channel(name)\n",
    "        print(f'Getting channel trend from {name}...', end='\\t')\n",
    "        sub_trend = self.get_trend(name, trend_type='sub_trend')\n",
    "        view_trend = self.get_trend(name, trend_type='view_trend')\n",
    "\n",
    "        start_date = str(view_trend['date'].iloc[0])[:10]\n",
    "        cumul_start_view = self.get_trend(name, trend_type='cumul_start_view', start_date=start_date)\n",
    "\n",
    "        result = self.wrap(sub_trend, view_trend, cumul_start_view)\n",
    "        file_name = f'{self.correct_file_name(name)}_trend.csv'\n",
    "        result.to_csv(os.path.join(self.save_path, file_name), index=False)\n",
    "        print(f'{file_name} saved.\\n')\n",
    "        self.driver.close()\n",
    "    \n",
    "    def get_trend(self, name, trend_type, start_date: '누적 조회수 수집에 사용'=None):\n",
    "        '''추이를 크롤링하는 함수'''\n",
    "        if trend_type == 'cumul_start_view':\n",
    "            cumul_start_view = self.grope(trend_type, start_date)\n",
    "            return cumul_start_view\n",
    "        \n",
    "        graph_elements = self.grope(trend_type)\n",
    "        if trend_type == 'sub_trend':\n",
    "            date_list = []\n",
    "            n_sub_list = []\n",
    "            for n in range(1, graph_elements['date_interval']):\n",
    "                self.move_cursor(n_offset=n, origin=graph_elements['start_point'], pix=graph_elements['pix'])\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                info = soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()\n",
    "                if '획기적' in info:\n",
    "                    try:\n",
    "                        try:\n",
    "                            date = (pd.to_datetime(date) + timedelta(1)).strftime('%Y-%m-%d')\n",
    "                        except:\n",
    "                            date = np.nan\n",
    "                    except:\n",
    "                        date = pd.to_datetime('1900-01-01')\n",
    "                    n_sub = self.calc_n_str(info.split(':')[-1].split('구독자 ')[-1])\n",
    "                else:\n",
    "                    date = info[:10]\n",
    "                    n_sub = self.calc_n_str(info[10:])\n",
    "                date_list.append(date)\n",
    "                n_sub_list.append(n_sub)\n",
    "\n",
    "            sub_trend = pd.DataFrame(dict(date=date_list, subscriber=n_sub_list))\n",
    "            sub_trend['date'] = pd.to_datetime(sub_trend['date'])\n",
    "            sub_trend = self.correct_timeline(sub_trend)\n",
    "            sub_trend = sub_trend.drop_duplicates(ignore_index=True)\n",
    "\n",
    "            return sub_trend\n",
    "        \n",
    "        elif trend_type == 'view_trend':\n",
    "            date_list = []\n",
    "            n_view_list = []\n",
    "            for n in range(1, graph_elements['date_interval']):\n",
    "                self.move_cursor(n_offset=n, origin=graph_elements['start_point'], pix=graph_elements['pix'])\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                info = soup.find('div', id=\"channel-history-view-chart\").find_all('div')[-1].get_text()\n",
    "                date = info[:10]\n",
    "                if '조회수' in info[10:]:\n",
    "                    n_view = self.calc_n_str(info[10:].strip().split('조회수')[0])\n",
    "                else:\n",
    "                    n_view = self.calc_n_str(info[10:])\n",
    "\n",
    "                date_list.append(date)\n",
    "                n_view_list.append(n_view)\n",
    "\n",
    "            view_trend = pd.DataFrame(dict(date=date_list, view=n_view_list))\n",
    "            view_trend['date'] = pd.to_datetime(view_trend['date'])\n",
    "            view_trend = self.correct_timeline(view_trend)\n",
    "            view_trend = view_trend.drop_duplicates(ignore_index=True)\n",
    "            \n",
    "            return view_trend\n",
    "        \n",
    "    def grope(self, trend_type, start_date:'cumul_view_start(누적 조회수 초기값)를 구할 때만 사용'=None):\n",
    "        '''추출할 트렌드의 날짜 범위, 그래프 내 좌표 범위를 추출하는 함수'''\n",
    "        if trend_type == 'sub_trend':\n",
    "            wait = WebDriverWait(self.driver, 10)\n",
    "            self.element = wait.until(lambda x: x.find_element_by_xpath('//*[@id=\"channel-history-sub-chart\"]/div[1]/canvas'))\n",
    "        elif trend_type == 'view_trend':\n",
    "            wait = WebDriverWait(self.driver, 10)\n",
    "            self.element = wait.until(lambda x: x.find_element_by_xpath('//*[@id=\"channel-history-view-chart\"]/div[1]/canvas'))\n",
    "        elif trend_type == 'cumul_start_view':\n",
    "            CUMUL = self.driver.find_element_by_xpath('//*[@id=\"tab-channel\"]/div[5]/div[1]/div/span[2]')\n",
    "            CUMUL.click()\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        # 그래프 위치 찾기\n",
    "        loc = self.element.location\n",
    "        size = self.element.size\n",
    "        origin = self.element.size['width'] // 2\n",
    "        self.move_cursor(n_offset=0, origin=origin)\n",
    "\n",
    "        start_origin = -origin + 60\n",
    "        end_origin = origin - 20\n",
    "        \n",
    "        if trend_type in ['sub_trend', 'view_trend']:\n",
    "            start_point, start_date = self.find_edges(start_origin, trend_type, 'start')\n",
    "            end_point, end_date = self.find_edges(end_origin, trend_type, 'end')\n",
    "            pix_interval = end_point - start_point\n",
    "            date_interval = (end_date - start_date).days\n",
    "            pix = pix_interval / date_interval\n",
    "            return dict(date_interval=date_interval, start_point=start_point, pix=pix)\n",
    "        \n",
    "        else: # 'cumul_start_view'\n",
    "            margin = 0\n",
    "            while True:\n",
    "                self.move_cursor(n_offset=0, origin=start_origin+12-margin)\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                info = soup.find('div', id=\"channel-history-view-chart\").find_all('div')[-1].get_text()\n",
    "                compare = info[:10]\n",
    "                if start_date == compare:\n",
    "                    cumul_start_view = self.calc_n_str(info[10:].strip().split('조회수')[0]) if '조회수' in info[10:] else self.calc_n_str(info[10:])\n",
    "                    break\n",
    "                margin += 1        \n",
    "            return cumul_start_view\n",
    "    \n",
    "    def find_edges(self, origin, trend_type, option, margin=-2) -> ('point', 'date'):\n",
    "        '''그래프의 끝과 끝 위치값을 탐색하는 함수'''\n",
    "        if trend_type == 'sub_trend':\n",
    "            \n",
    "            self.move_cursor(n_offset=0, origin=0)\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            standard = soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()[:10]\n",
    "            \n",
    "            if option=='start':\n",
    "                compare = None\n",
    "                while True:\n",
    "                    self.move_cursor(n_offset=0, origin=origin)\n",
    "                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                    temp = soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()\n",
    "                    compare = temp[:10]\n",
    "\n",
    "                    if compare != standard:\n",
    "                        if '획기적' not in compare: # '획기적 사건'이 아닌 일반적인 날짜\n",
    "                            origin += margin\n",
    "                            start_date = pd.to_datetime(soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()[:10])\n",
    "                            start_point = origin\n",
    "                            continue\n",
    "\n",
    "                        else: # '획기적 사건'이 나올 경우\n",
    "                            while True:\n",
    "                                self.move_cursor(n_offset=0, origin=origin)\n",
    "                                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                                pseudo_start = soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()[:10]\n",
    "                                start_point = origin\n",
    "                                if '획기적' not in pseudo_start:\n",
    "                                    start_date = pd.to_datetime(pseudo_start[:10]) - timedelta(1)\n",
    "                                    break\n",
    "                                else:\n",
    "                                    origin -= margin\n",
    "                                    continue\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                return start_point, start_date\n",
    "\n",
    "            else:\n",
    "                compare = None\n",
    "                while True:\n",
    "                    self.move_cursor(n_offset=0, origin=origin)\n",
    "                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                    temp = soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()\n",
    "                    compare = temp[:10]\n",
    "\n",
    "                    if compare != standard:\n",
    "                        if '획기적' not in compare: # '획기적 사건'이 아닌 일반적인 날짜\n",
    "                            origin -= margin\n",
    "                            end_date = pd.to_datetime(soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()[:10])\n",
    "                            end_point = origin\n",
    "                            continue\n",
    "\n",
    "                        else: # '획기적 사건'이 나올 경우\n",
    "                            while True:\n",
    "                                self.move_cursor(n_offset=0, origin=origin)\n",
    "                                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                                pseudo_end = soup.find_all('div', id=\"channel-history-sub-chart\")[0].get_text()[:10]\n",
    "                                end_point = origin\n",
    "                                if '획기적' not in pseudo_end:\n",
    "                                    end_date = pd.to_datetime(pseudo_end[:10]) - timedelta(1)\n",
    "                                    break\n",
    "                                else:\n",
    "                                    origin += margin\n",
    "                                    continue\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                return end_point, end_date\n",
    "\n",
    "        elif trend_type == 'view_trend':\n",
    "            self.move_cursor(n_offset=0, origin=0)\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            standard = soup.find('div', id=\"channel-history-view-chart\").find_all('div')[-1].get_text()[:10]\n",
    "            if option=='start':\n",
    "                compare = None\n",
    "                while True:\n",
    "                    self.move_cursor(n_offset=0, origin=origin)\n",
    "                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                    temp = soup.find('div', id=\"channel-history-view-chart\").find_all('div')[-1].get_text()\n",
    "                    compare = temp[:10]\n",
    "\n",
    "                    if compare != standard:\n",
    "                        origin += margin\n",
    "                        start_date = pd.to_datetime(compare)\n",
    "                        start_point = origin\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                return start_point, start_date\n",
    "\n",
    "            else:\n",
    "                compare = None\n",
    "                while True:\n",
    "                    self.move_cursor(n_offset=0, origin=origin)\n",
    "                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                    temp = soup.find('div', id=\"channel-history-view-chart\").find_all('div')[-1].get_text()\n",
    "                    compare = temp[:10]\n",
    "\n",
    "                    if compare != standard:\n",
    "                        origin -= margin\n",
    "                        end_date = pd.to_datetime(compare)\n",
    "                        end_point = origin\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                return end_point, end_date\n",
    "            \n",
    "    def move_cursor(self, n_offset, origin, pix=2):\n",
    "        action = webdriver.common.action_chains.ActionChains(self.driver)\n",
    "        action.move_to_element(self.element)\n",
    "        action.move_by_offset(origin + pix*n_offset, 0)\n",
    "        action.perform()\n",
    "        \n",
    "    def into_channel(self, channel_name):\n",
    "        CHANNEL = self.driver.find_element_by_xpath('//*[@id=\"header-search-input\"]')\n",
    "        CHANNEL.clear()\n",
    "        CHANNEL.send_keys(channel_name)\n",
    "        time.sleep(0.5)\n",
    "        CHANNEL.send_keys(Keys.ENTER)\n",
    "        \n",
    "        patience = 0\n",
    "        while True:\n",
    "            try:\n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                link = self.URL[:-1] + soup.find('div', class_='result').find('a', class_=\"channel-name ellipsis\", href=True)['href']\n",
    "                break\n",
    "            except: \n",
    "                if patience == 10:\n",
    "                    raise NotImplementedError()\n",
    "                time.sleep(0.5)\n",
    "                patience += 1\n",
    "        \n",
    "        compare = soup.find('a', class_='channel-name ellipsis').get_text().strip()\n",
    "        \n",
    "        if compare != channel_name.strip(): # 녹스에 채널이 등록되지 않은 경우\n",
    "            print(f\"Could not found channel '{channel_name.strip()}'\")\n",
    "            self.driver.close()\n",
    "            raise NameError()\n",
    "            \n",
    "        self.driver.get(link)\n",
    "        \n",
    "    def wrap(self, sub_trend, view_trend, cumul_start_view):\n",
    "        '''구독자 추이, 조회수 추이를 병합하는 함수'''\n",
    "        result = sub_trend.merge(view_trend, how='outer', on='date').sort_values(by='date', ignore_index=True)\n",
    "        result = self.imputate(result)\n",
    "        result['subscriber'] = result['subscriber'].astype(int)\n",
    "        result['view'] = result['view'].astype(int)\n",
    "        result = self.add_cumul_view(result, cumul_start_view)\n",
    "        return result\n",
    "        \n",
    "    @staticmethod\n",
    "    def add_cumul_view(data:'Trend 데이터', cumul_start_view: '누적 조회수 초기값'):\n",
    "        data['cumul_view'] = data['view'].cumsum() + cumul_start_view\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_n_str(str_n):\n",
    "        if '만' in str_n:\n",
    "            num = str_n.split('만')[0]\n",
    "            return int(float(num) * 10000)\n",
    "        elif '천' in str_n:\n",
    "            num = str_n.split('천')[0]\n",
    "            return int(float(num) * 1000)\n",
    "        else:\n",
    "            return int(float(str_n))\n",
    "        \n",
    "    @staticmethod\n",
    "    def correct_file_name(title):\n",
    "        invalid_file_name_list = ['\\\\', '/', ':', '*', '?', '\"', '<', '>', '|']\n",
    "        for inv in invalid_file_name_list:\n",
    "            if inv in title:\n",
    "                title = title.replace(inv, '')\n",
    "        return title\n",
    "\n",
    "    @staticmethod\n",
    "    def correct_timeline(data):\n",
    "        idx_to_correct = data[data['date'] == pd.to_datetime('1900-01-01')].index.tolist()\n",
    "        if idx_to_correct:\n",
    "            idx_shift = list(np.array(idx_to_correct) + 1)\n",
    "            data.loc[idx_to_correct, 'date'] = (data.loc[idx_shift, 'date'] - timedelta(1)).values\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def imputate(df):\n",
    "        detect = df.isnull().sum()\n",
    "        missing_cols = detect[detect != 0].index.tolist()\n",
    "        for col in missing_cols:\n",
    "            missing_idx = df[df[col].isnull()].index.tolist()\n",
    "            for m in missing_idx:\n",
    "                if m == 0:\n",
    "                    upper_fill = df.loc[m+1:, col]\n",
    "                    upper_bound = upper_fill[upper_fill.notnull()].tolist()[0]\n",
    "                    fill_value = int(upper_bound)\n",
    "                    pass\n",
    "                elif m == df.shape[0]-1:\n",
    "                    lower_fill = df.loc[:m-1, col]\n",
    "                    lower_bound = lower_fill[lower_fill.notnull()].tolist()[-1]\n",
    "                    fill_value = int(lower_bound)\n",
    "                else:\n",
    "                    lower_fill = df.loc[:m-1, col]\n",
    "                    upper_fill = df.loc[m+1:, col]\n",
    "                    lower_bound = lower_fill[lower_fill.notnull()].tolist()[-1]\n",
    "                    upper_bound = upper_fill[upper_fill.notnull()].tolist()[0]\n",
    "                    fill_value = int(np.mean([lower_bound, upper_bound]))\n",
    "                df.loc[m, col] = fill_value\n",
    "        return df\n",
    "    \n",
    "    \n",
    "class MetaCrawler:\n",
    "    def __init__(self, driver_path, save_path):\n",
    "        self.driver_path = driver_path\n",
    "        self.save_path = save_path\n",
    "    \n",
    "    def work(self, url: '수집할 채널의 url'):\n",
    "        channels_info = pd.DataFrame()\n",
    "        self.driver = webdriver.Chrome(self.driver_path)\n",
    "        self.driver.get(url) # 특정 채널로 이동\n",
    "        self.driver.maximize_window()\n",
    "\n",
    "        channel_info = self.get_channel_info() # 채널 정보 수집\n",
    "        if channel_info['channel'] == '채널 정지':\n",
    "            self.driver.close()\n",
    "            raise NotImplementedError()\n",
    "        channel_name = self.correct_file_name(channel_info['channel'])\n",
    "        self.driver.close()\n",
    "\n",
    "        meta = self.get_video_info(url) # 각 영상 정보 수집(댓글 내용X)\n",
    "        result = self.wrap(channel_info, meta)\n",
    "            \n",
    "        # 채널 하나에 대한 메타데이터 저장\n",
    "        file_name = f'{channel_name}_meta.csv'\n",
    "        meta.to_csv(os.path.join(self.save_path, file_name), index=False)\n",
    "        result.to_csv(os.path.join(self.save_path, file_name), index=False)\n",
    "        print(f\"'{file_name}' saved.\")\n",
    "        print(f\"All channels saved\")\n",
    "        \n",
    "    def get_channel_info(self):\n",
    "        '''채널 정보를 수집하는 함수(채널명, 가입일, 현재 누적 조회수)'''\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        if soup.find('yt-formatted-string', class_=\"style-scope yt-alert-renderer\"):\n",
    "            return pd.Series(dict(channel='채널 정지', sign_in='채널 정지', cumul_view='채널 정지')).to_frame().T\n",
    "        else:\n",
    "            self.click(option='channel_info')\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            channel_name = soup.find_all('yt-formatted-string', class_=\"style-scope ytd-channel-name\")[0].get_text()\n",
    "            sign_in = soup.find_all('yt-formatted-string', class_=\"style-scope ytd-channel-about-metadata-renderer\")[-2].get_text().split('가입일: ')[-1].replace(' ', '')\n",
    "            cumul_n_view = soup.find_all('yt-formatted-string', class_=\"style-scope ytd-channel-about-metadata-renderer\")[-1].get_text().split('조회수 ')[-1][:-1].replace(',', '')\n",
    "            try:\n",
    "                cumul_n_view = int(float(cumul_n_view))\n",
    "            except:\n",
    "                pass\n",
    "            return dict(channel=channel_name, sign_in=sign_in, total_cumul_view=cumul_n_view)\n",
    "\n",
    "    def get_video_info(self, url):\n",
    "        '''개별 영상 정보 수집'''\n",
    "        self.driver = webdriver.Chrome(self.driver_path)\n",
    "        self.driver.get(url)\n",
    "        video_link_list = self.get_video_list()\n",
    "        meta = pd.DataFrame() # 모든 영상 정보가 담길 데이터프레임\n",
    "        for video_url in tqdm(video_link_list):\n",
    "            self.driver.get(video_url)\n",
    "            patience = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    self.scroll_down(n=1, time_sleep=1)\n",
    "                    self.soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                    n_comment = self.get_n_comment(self.soup) # 댓글 수를 읽어온다\n",
    "                    break\n",
    "                except:\n",
    "                    if patience == 5:\n",
    "                        break\n",
    "                    patience += 1\n",
    "                    continue\n",
    "            if patience == 5:\n",
    "                continue\n",
    "            base = pd.Series(self.squeeze(n_comment)).to_frame().T\n",
    "            meta = pd.concat([meta, base], ignore_index=True)\n",
    "        self.driver.close()\n",
    "        return meta\n",
    "    \n",
    "    \n",
    "    def get_video_list(self):\n",
    "        '''채널 내 비디오 링크 수집'''\n",
    "        self.click(option='video')\n",
    "        self.scroll_down(n=100, time_sleep=1)\n",
    "        \n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        video_list = soup.select(selector='ytd-app')[0].find_all(class_='yt-simple-endpoint inline-block style-scope ytd-thumbnail')\n",
    "\n",
    "        ytb_link = 'https://www.youtube.com'\n",
    "        video_link_list = []\n",
    "        for idx in range(len(video_list)):\n",
    "            try:\n",
    "                link = video_list[idx]['href']\n",
    "                video_link_list.append(ytb_link + link)\n",
    "            except:\n",
    "                pass\n",
    "        print(f'Got {len(video_link_list)} video links.')\n",
    "        return video_link_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_n_comment(soup):\n",
    "        '''영상의 댓글 수를 탐지하는 함수'''\n",
    "        if soup.find_all('span', class_=\"style-scope yt-formatted-string\"):\n",
    "            is_stop = False\n",
    "            for s in soup.find_all('span', class_=\"style-scope yt-formatted-string\"):\n",
    "                if s.get_text() == '댓글이 사용 중지되었습니다. ':\n",
    "                    is_stop = True\n",
    "            if is_stop:\n",
    "                return '댓글 사용 중지'\n",
    "            else:\n",
    "                selector = 'ytd-comments > ytd-item-section-renderer > div > ytd-comments-header-renderer > div > h2'\n",
    "                n_comment = int(soup.select(selector)[0].get_text().split(' ')[-1].split('개')[0].replace(',', ''))\n",
    "                return n_comment\n",
    "        else: \n",
    "            selector = 'ytd-comments > ytd-item-section-renderer > div > ytd-comments-header-renderer > div > h2'\n",
    "            n_comment = int(soup.select(selector)[0].get_text().split(' ')[-1].split('개')[0].replace(',', '')) \n",
    "            return n_comment\n",
    "    \n",
    "    def squeeze(self, n_comment, selector='ytd-app > div > ytd-page-manager > ytd-watch-flexy'):\n",
    "        '''채널명, 구독자 수, 영상 제목, 조회수, 영상 길이, 게시일, 상세 정보, 댓글 수, 좋아요, 싫어요, 썸네일 url'''\n",
    "        temp = self.soup.select(selector)\n",
    "        try:\n",
    "            video_basic_info = json.loads(temp[0].find_all('script', class_=\"style-scope ytd-player-microformat-renderer\", id='scriptTag')[0].text)\n",
    "        except:\n",
    "            video_basic_info = json.loads(temp[0].find_all('script', class_=\"style-scope ytd-player-microformat-renderer\", id='scriptTag')[0].string)\n",
    "\n",
    "        channel_name = video_basic_info['author']\n",
    "        title = video_basic_info['name']\n",
    "        post_date = video_basic_info['uploadDate']\n",
    "        duration = round(int(video_basic_info['duration'].split('PT')[-1].split('S')[0]) / 60, 2)\n",
    "        description = video_basic_info['description']\n",
    "        thumbnail_url = video_basic_info['thumbnailUrl'][0]\n",
    "        genre = video_basic_info['genre']\n",
    "        isLive = video_basic_info.get('publication', False)\n",
    "        \n",
    "        # 실시간/최초공개 영상\n",
    "        if isLive: \n",
    "            try:\n",
    "                n_like = int(temp[0].find_all('yt-formatted-string')[5].get_attribute_list('aria-label')[0].split('좋아요 ')[-1][:-1].replace(',', ''))\n",
    "            except:\n",
    "                n_like = 0 # 좋아요/싫어요 숨김\n",
    "            try:\n",
    "                n_dislike = int(temp[0].find_all('yt-formatted-string')[6].get_attribute_list('aria-label')[0].split('싫어요 ')[-1][:-1].replace(',', ''))\n",
    "            except:\n",
    "                n_dislike = 0 # 좋아요/싫어요 숨김\n",
    "            n_subscribe = temp[0].find_all('yt-formatted-string', class_=\"style-scope ytd-video-owner-renderer\", id=\"owner-sub-count\")[0].get_text()\n",
    "            n_subscribe = self.calc_n_subscribe(n_subscribe)\n",
    "            n_view = temp[0].find_all('span', class_=\"view-count style-scope yt-view-count-renderer\")[0].get_text()\n",
    "            n_view = self.calc_n_view(n_view)\n",
    "        \n",
    "        # 그 외\n",
    "        else: # 그 외\n",
    "            try:\n",
    "                n_like = int(temp[0].find_all('yt-formatted-string', class_=\"style-scope ytd-toggle-button-renderer style-text\")[0].get_attribute_list('aria-label')[0].split('좋아요 ')[-1][:-1].replace(',', ''))\n",
    "            except:\n",
    "                n_like = 0 # 좋아요/싫어요 숨김\n",
    "            try:\n",
    "                n_dislike = int(temp[0].find_all('yt-formatted-string', class_=\"style-scope ytd-toggle-button-renderer style-text\")[1].get_attribute_list('aria-label')[0].split('싫어요 ')[-1][:-1].replace(',', ''))\n",
    "            except:\n",
    "                n_dislike = 0 # 좋아요/싫어요 숨김\n",
    "            n_subscribe = temp[0].find_all('yt-formatted-string', class_=\"style-scope ytd-video-owner-renderer\", id=\"owner-sub-count\")[0].get_text()\n",
    "            n_subscribe = self.calc_n_subscribe(n_subscribe)\n",
    "            n_view = temp[0].find_all('span', class_=\"view-count style-scope yt-view-count-renderer\")[0].get_text()\n",
    "            n_view = self.calc_n_view(n_view)\n",
    "        return dict(channel=channel_name, subscribe=n_subscribe, title=title, genre=genre, view=n_view, \n",
    "                    duration=duration, date=post_date, description=description, comment=n_comment, \n",
    "                    like=n_like, dislike=n_dislike, thumbnail=thumbnail_url)\n",
    "    \n",
    "    \n",
    "    def scroll_down(self, n, time_sleep):\n",
    "        '''브라우저의 스크롤을 내리는 함수'''\n",
    "        recent_point = 0\n",
    "        n_scroll = 0\n",
    "        while True:\n",
    "            end_point = self.driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            self.driver.execute_script(f\"window.scrollTo(0, {end_point});\")\n",
    "            time.sleep(time_sleep)\n",
    "            recent_point = self.driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            if (recent_point == end_point) or (n_scroll == n):\n",
    "                break\n",
    "            n_scroll += 1\n",
    "            \n",
    "            \n",
    "    def click(self, option):\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        n_tab = len(soup.find_all('paper-tab', class_=\"style-scope ytd-c4-tabbed-header-renderer\"))\n",
    "        if option=='video':\n",
    "            video = self.driver.find_element_by_xpath('//*[@id=\"tabsContent\"]/paper-tab[2]/div')\n",
    "            video.click()\n",
    "            time.sleep(1)\n",
    "        elif option=='channel_info':\n",
    "            info = self.driver.find_element_by_xpath(f'//*[@id=\"tabsContent\"]/paper-tab[{n_tab}]/div')\n",
    "            info.click()\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "    @staticmethod\n",
    "    def wrap(channel_info, meta):\n",
    "        meta['sign_in'] = channel_info['sign_in']\n",
    "        meta['total_cumul_view'] = channel_info['total_cumul_view']\n",
    "        return meta\n",
    "    \n",
    "    @staticmethod    \n",
    "    def correct_file_name(title):\n",
    "        invalid_file_name_list = ['\\\\', '/', ':', '*', '?', '\"', '<', '>', '|']\n",
    "        for inv in invalid_file_name_list:\n",
    "            if inv in title:\n",
    "                title = title.replace(inv, '')\n",
    "        return title\n",
    "    \n",
    "    @staticmethod    \n",
    "    def calc_n_subscribe(text):\n",
    "        mined_text = text.split('구독자 ')[-1]\n",
    "        if '천' in text:\n",
    "            return int(float(mined_text[:-2]) * 1000)\n",
    "        elif '만' in text:\n",
    "            return int(float(mined_text[:-2]) * 10000)\n",
    "        else:\n",
    "            try:\n",
    "                output = int(float(mined_text))\n",
    "            except:\n",
    "                output = int(float(mined_text[:-1]))\n",
    "            return output\n",
    "        \n",
    "    @staticmethod    \n",
    "    def calc_n_view(text):\n",
    "        return int(text.split('조회수 ')[-1][:-1].replace(',', ''))\n",
    "    \n",
    "    @staticmethod    \n",
    "    def calc_comment_like(text):\n",
    "        return int(text.split('명이')[0].split(' ')[-1].replace(',', ''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
