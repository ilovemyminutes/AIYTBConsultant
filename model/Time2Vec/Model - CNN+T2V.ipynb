{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "------------\n",
    "### 1. Environment & Framework\n",
    "The following code is implemented with PyTorch 1.7.1 under Ubuntu 18.04.5. <br>\n",
    "\n",
    "### <br> 2. Pipeline at a Glance\n",
    "The entire code is broken down into three parts.\n",
    "\n",
    "1. Dataset <br>\n",
    "```Dataset``` is to convert the raw ```.csv``` file into ```torch.Tensor```, and it also divide the original set into train and test sets. Default ratio for test set is set to 0.3. <br>\n",
    "<br>\n",
    "2. Builder <br>\n",
    "Without model itself, ```Builder``` is the core role in this implementation. ```Builder``` not only executes learning the model, but has some additional features to plot our prediction against ground truth. Considering that this work is a regression problem, visualization can be sometimes useful to acknowledge if the model is seemingly accurate. Though, ```Builder``` also outputs renouned metrics such as MAE.<br>\n",
    "<br>\n",
    "3. Model <br>\n",
    "Defines the models to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T03:17:33.191705Z",
     "start_time": "2020-11-20T03:17:33.186707Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, date\n",
    "from sklearn import utils\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, raw, test_size=0.3, random_state=42):\n",
    "        self.scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "        self.raw = raw.values #shape: (-1, f+t), ndarray\n",
    "        self.tr, self.tst = train_test_split(self.raw.copy(), test_size=test_size, random_state=random_state)\n",
    "        self.tr_M, self.tr_m = torch.from_numpy(self.tr.max(axis=1)).float(), torch.from_numpy(self.tr.min(axis=1)).float()\n",
    "        self.tst_M, self.tst_m = torch.from_numpy(self.tst.max(axis=1)).float(), torch.from_numpy(self.tst.min(axis=1)).float()\n",
    "        \n",
    "        for i in range(self.tr.shape[0]):\n",
    "            self.tr[i] = self.scaler.fit_transform(self.tr[i].reshape(-1,1)).reshape(1,-1)\n",
    "        for i in range(self.tst.shape[0]):\n",
    "            self.tst[i] = self.scaler.fit_transform(self.tst[i].reshape(-1,1)).reshape(1,-1)\n",
    "            \n",
    "        self.tr, self.tst = torch.from_numpy(self.tr).float(), torch.from_numpy(self.tst).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Builder:\n",
    "    def __init__(self, model, dataset, f, t):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.f, self.t = f, t\n",
    "        self.tr_loss, self.val_loss = [], []\n",
    "    \n",
    "    \n",
    "    def _mini_batch(self, x, B, shuffle=True): #shape: (-1, f+t), tensor\n",
    "        batch = []\n",
    "        if (x.size(0) % B) == 0:\n",
    "            iteration = x.size(0) // B\n",
    "        else:\n",
    "            iteration = x.size(0) // B + 1\n",
    "        \n",
    "        order = np.arange(x.size(0))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(order)\n",
    "            \n",
    "        for i in range(iteration):\n",
    "            indices = order[B*i : B*(i+1)]\n",
    "            batch.append(x[indices])\n",
    "        return batch #shape: (iteration), element_shape: (B, f+t)\n",
    "    \n",
    "    \n",
    "    ####################TRAIN####################\n",
    "    def fit(self, optimizer=None, loss_fn=None, batch_size=128, lr=1e-3, epochs=10, val_ratio=0.1):\n",
    "        # mini batch -> val -> train -> print\n",
    "        data_load = self._mini_batch(self.dataset.tr, batch_size, shuffle=True)\n",
    "        tr_load = data_load[int(len(data_load)*val_ratio):]\n",
    "        val_load = data_load[:int(len(data_load)*val_ratio)]\n",
    "        \n",
    "        loss_fn = loss_fn()\n",
    "        opt = optimizer(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        for ep in range(epochs): #x_shape: (B, f), y_shape: (B, t)\n",
    "            tr_loss_bin, val_loss_bin = [], []\n",
    "            \n",
    "            with torch.no_grad(): #cross validate\n",
    "                for i, val_data in enumerate(tqdm(val_load)): \n",
    "                    x, y = val_data[:, :self.f], val_data[:, self.f:]\n",
    "                    self.model.eval()\n",
    "                    pred = self.model(x.to(device)) # GPU\n",
    "                    loss = loss_fn(pred, y.to(device))\n",
    "                    val_loss_bin.append(loss.item())\n",
    "                    \n",
    "            for i, tr_data in enumerate(tqdm(tr_load)): # train\n",
    "                x, y = tr_data[:, :self.f], tr_data[:, self.f:]\n",
    "                self.model.train()\n",
    "                pred = self.model(x.to(device)) # GPU\n",
    "                loss = loss_fn(pred, y.to(device))\n",
    "                tr_loss_bin.append(loss.item())\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            \n",
    "            self.tr_loss.append(np.mean(tr_loss_bin))\n",
    "            self.val_loss.append(np.mean(val_loss_bin))\n",
    "            print({scheduler.get_lr()})\n",
    "            print(f'Epoch: {ep+1}/{epochs}, Train Loss: {self.tr_loss[-1]:.4f}, Val Loss: {self.val_loss[-1]:.4f}')\n",
    "    \n",
    "    \n",
    "    ####################EVALUATION####################\n",
    "    def eval(self):\n",
    "        MAE = nn.L1Loss()\n",
    "        MSE = nn.MSELoss()\n",
    "        MAE_bin, MSE_bin = [], []\n",
    "        data_load = self._mini_batch(self.dataset.tst, 128, shuffle=False)\n",
    "        M_load = self._mini_batch(self.dataset.tst_M, 128, shuffle=False)\n",
    "        m_load = self._mini_batch(self.dataset.tst_m, 128, shuffle=False)\n",
    "        \n",
    "        for i, data in enumerate(tqdm(data_load)):\n",
    "            x, y = data[:, :self.f], data[:, self.f:] #(iteration), (B, f+t)\n",
    "            M, m = M_load[i].view(-1,1), m_load[i].view(-1,1) #(iteration), (B, 1)\n",
    "            self.model.eval()\n",
    "            pred = self.model(x.to(device)).to('cpu')            \n",
    "            pred, y = (M-m)*pred + m, (M-m)*y + m\n",
    "            MAE_bin.append(MAE(pred, y).item())\n",
    "            MSE_bin.append(MSE(pred, y).item())\n",
    "        MAE_result = np.mean(MAE_bin)\n",
    "        RMSE_result = np.sqrt(np.mean(MSE_bin))\n",
    "        print(f'MAE Loss: {MAE_result:.2f}, RMSE Loss: {RMSE_result:.2f}')\n",
    "\n",
    "    \n",
    "    ####################VISUALIZATION####################\n",
    "    def plot(self, row, col, train=False):\n",
    "        if train:\n",
    "            order = np.arange(self.dataset.tr.size(0))\n",
    "            np.random.shuffle(order)\n",
    "            order = order[:row*col]\n",
    "            #shape: (row*col, f+t)\n",
    "            data, M, m = self.dataset.tr[order], self.dataset.tr_M[order].view(-1,1), self.dataset.tr_m[order].view(-1,1)\n",
    "        else:\n",
    "            order = np.arange(self.dataset.tst.size(0))\n",
    "            np.random.shuffle(order)\n",
    "            order = order[:row*col]\n",
    "            #shape: (row*col, f+t)\n",
    "            data, M, m = self.dataset.tst[order], self.dataset.tst_M[order].view(-1,1), self.dataset.tst_m[order].view(-1,1)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            x, y = data[:, :self.f], data[:, self.f:] #x_shape: (row*col, f), y_shape: (row*col, t)\n",
    "            pred = self.model(x.to(device)).to('cpu') #pred_shape: (row*col, t)\n",
    "            pred, y = (M-m)*pred + m, (M-m)*y + m\n",
    "            \n",
    "        fig = plt.figure(figsize=(20,15))\n",
    "        for i in range(row*col):\n",
    "            ax = plt.subplot(row, col, i+1)\n",
    "            ax.plot(np.arange(self.t)+1, pred[i], label='Prediction')\n",
    "            ax.plot(np.arange(self.t)+1, y[i], label='Truth')\n",
    "            ax.legend(loc='upper left')\n",
    "        \n",
    "        \n",
    "    def learning_curve(self):\n",
    "        fig = plt.figure(figsize=(10,5))\n",
    "        plt.title('Learning Curve')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot(np.arange(len(self.tr_loss))+1, self.tr_loss, label='Train')\n",
    "        plt.plot(np.arange(len(self.val_loss))+1, self.val_loss, label='Val')\n",
    "        plt.xlim((1, len(self.tr_loss)))\n",
    "        plt.legend()\n",
    "    \n",
    "    \n",
    "    ####################SAVE & LOAD MODEL####################\n",
    "    def save_model(self, name):\n",
    "        torch.save(self.model.state_dict(), os.path.join('./models', name+'.pt'))\n",
    "        np.save(os.path.join('./history', name+' (Train)'), self.tr_loss)\n",
    "        np.save(os.path.join('./history', name+' (Val)'), self.val_loss)\n",
    "        \n",
    "    \n",
    "    def load_model(self, name):\n",
    "        self.model.load_state_dict(torch.load(os.path.join('./models', name+'.pt')))\n",
    "        self.tr_loss = list(np.load(os.path.join('./history', name+' (Train).npy')))\n",
    "        self.val_loss = list(np.load(os.path.join('./history', name+' (Val).npy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fourier(nn.Module):\n",
    "    def __init__(self, depth, t):\n",
    "        super().__init__()\n",
    "        self.A = nn.parameter.Parameter(torch.rand(depth, t))\n",
    "        self.B = nn.parameter.Parameter(torch.rand(depth, t))\n",
    "        self.C = nn.parameter.Parameter(torch.rand(depth, t))\n",
    "        \n",
    "    def forward(self, x): # in:(B, detph, t), out:(B, t)\n",
    "        x = torch.mul(x, self.B)\n",
    "        x = torch.add(x, self.C)\n",
    "        x = torch.sin(x)\n",
    "        x = torch.mul(x, self.A)\n",
    "        x = torch.sum(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, filter_size, target_size, depth=32):\n",
    "        super().__init__()\n",
    "        self.f = filter_size\n",
    "        self.t = target_size\n",
    "        self.k = filter_size - target_size + 1\n",
    "        self.d = depth\n",
    "        self.conv = nn.Conv1d(1, depth, self.k, 1) #in: (B, 1, f), #out: (B, depth, t)\n",
    "        self.fourier = fourier(depth, t) # in:(B, detph, t), out:(B, t)\n",
    "        \n",
    "    def forward(self, x): #in: (B, f)\n",
    "        x = x.unsqueeze(1) # (B, 1, f)\n",
    "        x = self.conv(x) # (B, depth, t)\n",
    "        x = self.fourier(x) # (B, t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Execution & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t = 90, 30\n",
    "raw = pd.read_csv(f'./data/Data_({f},{t})').iloc[:,1:]#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(f, t, depth=32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "builder = Builder(model, dataset, f, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "builder.fit(optimizer=optim.Adam, loss_fn=nn.L1Loss, batch_size=128, lr=1e-5, epochs=50, val_ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "builder.learning_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "builder.plot(6,2, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "builder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#builder.save_model(f'CNN+T2V - ({f},{t})')\n",
    "#builder.load_model(f'CNN+T2V - ({f},{t})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cancer",
   "language": "python",
   "name": "cancer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
