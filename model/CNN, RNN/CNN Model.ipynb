{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! jt -t chesterish # chesterish 테마로 변경합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load(\n",
    "    filter_size: '60, 90, 180', \n",
    "    target_size: '1, 7, 30, 180', \n",
    "    stride: '1, 2, 3',\n",
    "    drop_suffix: '각 변수 끝에 붙은 번호를 제거할지 여부'=True,\n",
    "    path='/Users/Namin/Documents/CAP/CapstoneUOS/Data/data_variants',) -> (pd.DataFrame, pd.DataFrame):\n",
    "    \n",
    "    print(f'Setting: filter_size({filter_size})\\ttarget_size({target_size})\\tstride({stride})\\tdrop_suffix({drop_suffix})')\n",
    "    X_name = f'fs({filter_size})_ts({target_size})_st({stride}).csv'\n",
    "    y_name = f'fs({filter_size})_ts({target_size})_st({stride})_label.csv'\n",
    "    \n",
    "    print('Load feature data...', end='\\t')\n",
    "    X = pd.read_csv(os.path.join(path, X_name))\n",
    "    print('loaded!')\n",
    "    print('Load label data...', end='\\t')\n",
    "    y = pd.read_csv(os.path.join(path, y_name))\n",
    "    print('loaded!')\n",
    "    \n",
    "    if drop_suffix:\n",
    "        X.columns = list(map(lambda x: x.split('.')[0], X.columns.tolist()))\n",
    "        y.columns = list(map(lambda x: x.split('.')[0], y.columns.tolist()))\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size = 180\n",
    "target_size = 90\n",
    "stride = 1\n",
    "X, y = load(filter_size, target_size, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['n_hashtage', 'video_num', 'no_upload_interval',\n",
    "            'duration', 'video_n_view', 'n_dislike', 'n_like', 'n_comment',\n",
    "            'view_diff',\n",
    "            'sub_diff']\n",
    "target_features = ['sub_diff']\n",
    "\n",
    "X = X[features]\n",
    "y = y[target_features]\n",
    "y = pd.DataFrame(y.iloc[:,-1], columns=['sub_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "random_state = 42\n",
    "test_size = 0.2\n",
    "\n",
    "X_trn, X_test, y_trn, y_test = train_test_split(\n",
    "    X, y, \n",
    "    shuffle=True, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state\n",
    ") \n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_trn, y_trn, \n",
    "    shuffle=True, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state\n",
    ") \n",
    "\n",
    "### 스케일링 필요 시 다음을 진행(타깃에 대한 스케일링은 진행되지 않음)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_valid = pd.DataFrame(scaler.transform(X_valid), columns=X_valid.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train's shape: \", X_train.shape) \n",
    "print(\"y_train's shape: \", y_train.shape) \n",
    "print(\"X_valid's shape: \", X_valid.shape)\n",
    "print(\"y_valid's shape: \", y_valid.shape)\n",
    "print(\"X_test's shape: \", X_test.shape)\n",
    "print(\"y_test's shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 구성 살피기\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "h = 3 # graph's height(length)\n",
    "\n",
    "for n in range(10): # 1부터 x_train.shape[0]까지\n",
    "    fig = plt.figure(figsize=(h*len(set(X_train.columns.tolist())), h))\n",
    "    for i, train_var in enumerate(set(X_train.columns.tolist())):\n",
    "        plt.subplot(1, len(set(X_train.columns.tolist())), i+1)\n",
    "        plt.plot(X_train[train_var].iloc[n].values)\n",
    "        plt.title(str(train_var))\n",
    "\n",
    "    fig = plt.figure(figsize=(h*len(set(y_train.columns.tolist())), h))\n",
    "    for i, label_var in enumerate(set(y_train.columns.tolist())):\n",
    "        plt.subplot(1, len(set(y_train.columns.tolist())), i+1)\n",
    "        plt.plot(y_train[label_var].iloc[n], marker='o')\n",
    "        plt.title(\"GOAL: \" + str(label_var))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 10 # number of train features\n",
    "n2 = 1 # number of target features\n",
    "\n",
    "x_train_copy = X_train.fillna(0).values.reshape(X_train.shape[0], n1, -1).swapaxes(1, 2)\n",
    "y_train_copy = y_train.fillna(0).values.reshape(y_train.shape[0], n2, -1).swapaxes(1, 2)\n",
    "x_val_copy = X_valid.fillna(0).values.reshape(X_valid.shape[0], n1, -1).swapaxes(1, 2)\n",
    "y_val_copy = y_valid.fillna(0).values.reshape(y_valid.shape[0], n2, -1).swapaxes(1, 2)\n",
    "x_test_copy = X_test.fillna(0).values.reshape(X_test.shape[0], n1, -1).swapaxes(1, 2)\n",
    "y_test_copy = y_test.fillna(0).values.reshape(y_test.shape[0], n2, -1).swapaxes(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train_copy's shape: \", x_train_copy.shape) \n",
    "print(\"y_train_copy's shape: \", y_train_copy.shape) \n",
    "print(\"x_val_copy's shape: \", x_val_copy.shape)\n",
    "print(\"y_val_copy's shape: \", y_val_copy.shape)\n",
    "print(\"x_test_copy's shape: \", x_test_copy.shape)\n",
    "print(\"y_test_copy's shape: \", y_test_copy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 만들기\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, Conv1D, MaxPooling1D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "filters = len(features)\n",
    "output_num = len(target_features)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Dense(8, activation='softmax'),\n",
    "          input_shape=(x_train_copy.shape[-2:])))\n",
    "model.add(Conv1D(filters=16, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(16))\n",
    "model.add(Dense(output_num))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "model.compile(loss=['mae'],\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "              optimizer=Adam(lr=0.0001))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 기록하기\n",
    "import time\n",
    "\n",
    "filename='cnn_{}_{}_{}_2features.h5'.format(filter_size, target_size, stride)\n",
    "\n",
    "callback_list = [tf.keras.callbacks.ModelCheckpoint(filepath='Checkpoint/{}'.format(filename),\n",
    "                                                    monitor='val_loss',\n",
    "                                                    verbose=1,\n",
    "                                                    save_best_only=True,\n",
    "                                                    mode='min'),\n",
    "                 #tf.keras.callbacks.TensorBoard(log_dir='TensorBoard/cnn/{}'.format(time.asctime())),\n",
    "                 tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  patience=5),\n",
    "                 #tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                 #                                     factor=0.1,\n",
    "                 #                                     min_lr=1e-4, \n",
    "                 #                                     patience=0)\n",
    "                ]\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32 \n",
    "\n",
    "history2 = model.fit(x_train_copy, y_train_copy,\n",
    "                     epochs=EPOCHS,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     validation_data=(x_val_copy, y_val_copy),\n",
    "                     callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATED_EPOCHS = 7 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 성능 그래프로 나타내기\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "epochs = np.arange(1, UPDATED_EPOCHS)\n",
    "fig, axes = plt.subplots(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history2.history['loss'], label='train_loss')\n",
    "plt.plot(epochs, history2.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('epochs', fontdict={'size': 10})\n",
    "plt.ylabel('loss', fontdict={'size': 10})\n",
    "plt.title(\"MAE\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history2.history['root_mean_squared_error'], label='train_loss')\n",
    "plt.plot(epochs, history2.history['val_root_mean_squared_error'], label='val_loss')\n",
    "plt.xlabel('epochs', fontdict={'size': 10})\n",
    "plt.ylabel('loss', fontdict={'size': 10})\n",
    "plt.title(\"RMSE\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장하고 복원하기\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "filename = 'cnn_{}_{}_{}_2features.h5'.format(filter_size, target_size, stride)\n",
    "\n",
    "model.save('Model/{}'.format(filename))\n",
    "res_model = load_model('Model/{}'.format(filename))\n",
    "res_model.load_weights('Model/{}'.format(filename))\n",
    "#res_model.evaluate(x_test_copy, y_test_copy)\n",
    "mae, rmse = model.evaluate(x_test_copy, y_test_copy, verbose=2)\n",
    "print(\"MAE of Untrained Model: {:5.2f}%\".format(mae))\n",
    "print(\"RMSE of Untrained Model: {:5.2f}%\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 세트로 예측하기\n",
    "test_predict = model.predict(x_test_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측한 것 시각화하기 (한 개 다 합친 것)\n",
    "\n",
    "actual = pd.DataFrame(y_test_copy[:,0,0], columns=['actual'])\n",
    "pred = pd.DataFrame(test_predict[:,0], columns=['pred'])\n",
    "result = pd.concat([actual, pred], axis=1)\n",
    "ord_result = result.sort_values(by=['actual']) # 순차적으로\n",
    "start, end = 7500, 8000\n",
    "my_result = ord_result.iloc[start:end] # 보고 싶은 범위만\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(20, 15))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "total = [j for j in range(y_test_copy.shape[0])]\n",
    "plt.plot(total, result['actual'], 'r', label='actual')\n",
    "plt.plot(total, result['pred'], 'b', label='predicted', alpha=0.7)\n",
    "plt.title(\"All of the Test Datas\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "total = [j for j in range(ord_result.shape[0])]\n",
    "plt.plot(total, ord_result['actual'], 'r', label='actual')\n",
    "plt.plot(total, ord_result['pred'], 'b', label='predicted', alpha=0.7)\n",
    "plt.title(\"All of the Test Datas (ordered)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "total = [j for j in range(my_result.shape[0])]\n",
    "plt.plot(total, my_result['actual'], 'r', label='actual')\n",
    "plt.plot(total, my_result['pred'], 'b', label='predicted', alpha=0.7)\n",
    "plt.title(\"Some Test Datas (ordered)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_a, good_p, bad_a, bad_p = [], [], [], []\n",
    "mae = int(mae)\n",
    "\n",
    "for i in range(result.shape[0]):\n",
    "    a = result['actual'].iloc[i]\n",
    "    p = result['pred'].iloc[i]\n",
    "\n",
    "    if abs(a-p) <= mae:\n",
    "        good_a.append(a)\n",
    "        good_p.append(p)\n",
    "    else:\n",
    "        bad_a.append(a)\n",
    "        bad_p.append(p)\n",
    "\n",
    "good_a = pd.DataFrame(good_a, columns=['actual'])\n",
    "good_p = pd.DataFrame(good_p, columns=['pred'])\n",
    "good_result = pd.concat([good_a, good_p], axis=1)\n",
    "good_ord_result = good_result.sort_values(by=['actual']) # 순차적으로\n",
    "\n",
    "bad_a = pd.DataFrame(bad_a, columns=['actual'])\n",
    "bad_p = pd.DataFrame(bad_p, columns=['pred'])\n",
    "bad_result = pd.concat([bad_a, bad_p], axis=1)\n",
    "bad_ord_result = bad_result.sort_values(by=['actual']) # 순차적으로\n",
    "\n",
    "print(\"total test samples:\", result.shape[0], \"개\")\n",
    "per_mae = np.round((good_result.shape[0]/result.shape[0])*100, 2)\n",
    "print(\"good mae result / total result (%):\", per_mae)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(20, 20)) # Test MAE보다 작거나 같은 것만\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "total = [j for j in range(good_result.shape[0])]\n",
    "plt.plot(total, good_result['actual'], 'r', label='actual')\n",
    "plt.plot(total, good_result['pred'], 'b', label='predicted', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\"Good Predictions (error less than MAE {}\".format(mae)+ \")\")\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "total = [j for j in range(good_ord_result.shape[0])]\n",
    "plt.plot(total, good_ord_result['actual'], 'r', label='actual')\n",
    "plt.plot(total, good_ord_result['pred'], 'b', label='predicted', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\"Good Predictions (error less than MAE {}\".format(mae)+ \") (ordered)\")\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "total = [j for j in range(bad_result.shape[0])]\n",
    "plt.plot(total, bad_result['actual'], 'r', label='actual')\n",
    "plt.plot(total, bad_result['pred'], 'b', label='predicted', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\"Bad Predictions (error more than MAE {}\".format(mae)+ \")\")\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "total = [j for j in range(bad_ord_result.shape[0])]\n",
    "plt.plot(total, bad_ord_result['actual'], 'r', label='actual')\n",
    "plt.plot(total, bad_ord_result['pred'], 'b', label='predicted', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\"Bad Predictions (error ore than MAE {}\".format(mae)+ \") (ordered)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 40000\n",
    "end = 45000\n",
    "\n",
    "column_classes = X.columns.unique().tolist()\n",
    "channel = pd.DataFrame(scaler.transform(X.loc[start:end, :]), columns=X.columns)\n",
    "channel_list = channel.values.reshape(channel.shape[0], filter_size, -1) # (end-start, features, filters)\n",
    "\n",
    "channel_real = y.loc[start:end, :].values.flatten() # (end-start, 1)\n",
    "channel_pred = model.predict(channel_list).flatten() # (end-start, 1)\n",
    "pred = pd.DataFrame(dict(real=channel_real, pred=channel_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "to_plot = pred.melt()\n",
    "to_plot['ticks'] = [i for i in range(pred.shape[0])] * 2\n",
    "sns.lineplot(x='ticks', y='value', hue='variable', data=to_plot)\n",
    "plt.title('Prediction for One Channel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
